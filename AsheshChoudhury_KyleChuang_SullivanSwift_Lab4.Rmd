---
title: "Statistical Methods for Discrete Response, Time Series, and Panel Data (W271): Lab 4"
author: "W271 Instructional Team"
date: "Fall 2018"
output:
  pdf_document: default
  html_notebook: default
---

page limit: 20 pages

```{r include=FALSE}
plottot = function(variable){
  #g=hist(data[variable],main='Histogram of Total Fatality Rate',xlab='Fatalities/population',ylab='Count')
  g=hist(data[variable],main=variable,xlab='Fatalities/population',ylab='Count')
  print(g)

  tmp=data
  tmp['var']=data[,eval(variable)]
  g=ggplot(tmp,aes(y=var,x=factor(year),group=factor(state),color=factor(state)))+geom_line()+theme(axis.text.x = element_text(angle = 45, hjust = 1))+labs(x=c('Year'),y=c('Fatalities/population'),title=c('Fatalies/Population by Year by State'),color=c('State'))+theme(plot.title = element_text(hjust = 0.5))
  print(g)
  
  g=ggplot(tmp,aes(y=var,x=factor(state),fill=factor(state)))+geom_boxplot()+theme(axis.text.x = element_text(angle = 45, hjust = 1))+labs(x=c('Year'),y=c('Fatalities/population'),title=c('Fatalies/Population by Year'),fill=c('Year'))+theme(plot.title = element_text(hjust = 0.5))+guides(fill=FALSE) 
  print(g)
  
  g=ggplot(tmp,aes(y=var,x=factor(year),fill=factor(year)))+geom_boxplot()+theme(axis.text.x = element_text(angle = 45, hjust = 1))+labs(x=c('Year'),y=c('Fatalities/population'),title=c('Fatalies/Population by Year'),fill=c('Year'))+theme(plot.title = element_text(hjust = 0.5))+guides(fill=FALSE) 
  print(g)
}

plotmix = function(dataframe,variable,text){
  tmp['var']=tmp[,eval(variable)]
  g=ggplot(tmp,aes(y=totfatrte,x=year,fill=var))+geom_bar(stat='identity')+labs(x='Year',y='Fatalities/population',title=paste('Fatalities/Population vs ',text,' Laws'))+theme(plot.title = element_text(hjust = 0.5))+guides(fill=guide_legend(title=text))
  print(g)
}

density_plot = function(data, plotvar, title) {
  ggplot(data, aes(plotvar)) + geom_density() + ggtitle(title)
}

conditional_plot = function(data, plotvar, condvar, title) {
  g <- ggplot(data, aes(as.factor(condvar), plotvar)) 
  g + geom_boxplot() + ggtitle(title)
}


ts_resid = function(x){
  par(mfrow=c(2,2),par=c(2,2,2,2))
  plot(x,main='t-Plot')
  qqnorm(x)
  qqline(x,col='red')
  acf(x)
  pacf(x)
  d=data.frame(PhillipsPerron=pp.test(x)$p.value,
               AugmentedDickeyFuller=adf.test(x)$p.value,
               LjungBox=Box.test(x,type='Ljung-Box')$p.value,
               ShapiroWilkes=shapiro.test(x)$p.value)
  xtable(d)
  
  ShapiroWilks <- shapiro.test(x)$p.value
}
```

```{r include=FALSE}
library(Hmisc)
library(car)
library(ggplot2)
library(ggrepel)
library(plm)
library(tidyr)
library(dplyr)
#rm(list=ls())
```

```{r}
load('driving.RData')
```

(Questsion 1) The dataset contains about 1200 observations ranging from 1980 to 2004 for the 48 contentential states. The observed variables include: Speed limits (`slXX`) seat belt and zero tolerance laws, graduated driver, blood alcohol level (`bacXX`), per se are in percent of year by months in binary. `Sbl70plus`, `sbprim`, `sbsecon` and `dXX` variables are simply derivatives or dummy variables of the other variables in the data set. Other variables are continuous with a a base of 0 and no top coding, except perc14_24 (100%).

Our research question is whether or not traffic laws can affect total fatalities. Total fatalities is a function of population, vehicle miles, traffic laws and unobservable variables. Our dataset contains 9 fatality-related variables, some normalized in various ways. We will not consider the weekend and night fatality variables as we are focusing on total fatalities and not when they occurred.

```{r}
ggplot(data,aes(y=statepop,x=factor(year),group=factor(state),color=factor(state)))+geom_line()+theme(axis.text.x = element_text(angle = 45, hjust = 1))+labs(x=c('Year'),y=c('Fatalities/population'),title=c('Fatalies/Population by Year by State'),color=c('State'))+theme(plot.title = element_text(hjust = 0.5))
```

Three states, 5, 44 and 10, significantly increased in population while other states were relatively flat. This suggests regressing against a population normalized fatality measure, such as totfatrte, would be most useful for examining causing inferences of state driving laws.

```{r}
ggplot(data,aes(y=vehicmiles,x=factor(year),group=factor(state),color=factor(state)))+geom_line()+theme(axis.text.x = element_text(angle = 45, hjust = 1))+labs(x=c('Year'),y=c('Fatalities/population'),title=c('Fatalies/Population by Year by State'),color=c('State'))+theme(plot.title = element_text(hjust = 0.5))
```

Vehicle miles has roughly similar trends for almost all states. This further confirms totfatrte as a dependent variable for traffic law causal inference as vehicle miles is more "stable" among the states than population through out time.

```{r fig.height=3}
plottot('totfatrte')
```

## HISTOGRAM ISN'T DISPLAYING TITLES

Mean fatalities/population has dropped by about 10% from 1980 to 2004. State 51 has persistently stayed near the top of the fatalies while State 38 seems to stay near the bottom. The range of fatalities changes throughout time, with the highest fatality rate dropping from over 50% in 1980 to under 45% in 2004. The minimum values for fatalities drop at well, but less overall, from about just over $10$ in 1980 to just under $10$ in 2004.

```{r}
# I merged this back into the totfatrte with ggplot
#conditional_plot(data, data$totfatrte, data$state, "Total Fatality Rate per 100,000 population by State")
```

There are several states that have higher overall fatality rates than the others. States 25, 32 and 51 may be further explored to see the relationships with the observed variables.

```{r}
tmp=data %>% select(year,totfatrte,sl55,sl65,sl70,sl75,slnone) %>% group_by(year) %>% summarize_all(funs(mean)) 
tmp=tmp$totfatrte*tmp
tmp$year=tmp$year/sqrt(tmp$totfatrte)
tmp$totfatrte=NULL
rownames(tmp)=tmp$year
tmp$year=NULL
tmp=t(tmp)
barplot(as.matrix(tmp),col=c('cadetblue1','green','brown','yellow','blue'),xlab='Year',ylab='Fatalities/population',main='Fatalities/population by year and Speed Limit')
legend('bottomleft',legend=rownames(tmp),col=c('cadetblue1','green','brown','yellow','blue'),lty=c(1,1,1,1))
```

## SS I'm a little confused as to who these plot are put together. same question on all stacked-like bar charts. Could we view stacked bar charts of the Speed limits and impose and average line for `totfatrte` on top

Initially, the speed limit (`slXX`) increase does not seem to affect the average total fatality rate across states immediately. In 1986-1987, speed limits increased in many states and in 1988-1991, fatalities fell by about 10%. The variable may be a candidate as an interaction variable with time. 

```{r}
tmp = data %>% select(year,totfatrte,sl70plus) %>% group_by(year) %>% summarize_all(funs(mean)) %>% mutate(sl70under=1-sl70plus)
tmp=tmp$totfatrte*tmp
tmp$year=tmp$year/sqrt(tmp$totfatrte)
tmp$totfatrte=NULL
rownames(tmp)=tmp$year
tmp$year=NULL
tmp=t(tmp)
barplot(as.matrix(tmp),col=c('blue','red'),xlab='Year',ylab='Fatalities/population',main='Fatalities/population by year and Speed Limit > 70+')
legend('bottomleft',legend=rownames(tmp),col=c('blue','red'),lty=c(1,1,1,1))
```

To further examine speed limits, we split the variables into two groups - states with speed limits under 70 and states with speed limits over 70. From the chart, it appears that speed limit over 70+ does not impact fatalities in a significant way.

```{r}
tmp = data %>% select(year,totfatrte,bac10,bac08) %>% group_by(year) %>% summarize_all(funs(mean)) %>% mutate(bac0=1-bac10-bac08) 

tmp=tmp$totfatrte*tmp
tmp$year=tmp$year/sqrt(tmp$totfatrte)
tmp$totfatrte=NULL
rownames(tmp)=tmp$year
tmp$year=NULL
tmp=t(tmp)
barplot(as.matrix(tmp),col=c('blue','green','yellow'),xlab='Year',ylab='Fatalities/100k vehicle miles',main='Fatalities/100k vehicle mile by year and Speed Limit')
legend('topright',legend=rownames(tmp),col=c('blue','green','yellow'),lty=c(1,1,1,1))
```

`bac0` represents where states did not have laws relating to blood alcohol content.

Blood Alcohol Content laws appear to have an immediate impact on fatalities. More interestingly, the graph suggests transformig the BAC laws into a binary variabe, as a `bac08` and `bac10` does not appear to affect fatalities, but the initial implementation of drinking-related laws has an impact. 

We will performed t-tests for 2001 and 2002 when the `bac08` and `bac10` are closest to 50% between the states. The two t-tests are performed to avoid the general downward trend of fatalities impacting the analysis. In both t-tests, the $H_0$: differences in means $=0$ are not rejected. From the graph above, there aren't likely to be lagged effects for `bac10` and `bac08` as `bac10` decreased from 1997-2004, but fatalities appear roughly the same. The lack of lagged effect makes sense as bac laws will immediately impact drunk driver and remove theme from the roads.

```{r}
#t.test(data[data$year==2001 & data$bac10==1,"totfatrte"],data[data$year==2001 & data$bac08==1,'totfatrte'])
#t.test(data[data$year==2002 & data$bac10==1,"totfatrte"],data[data$year==2002 & data$bac08==1,'totfatrte'])  
```

We may later test this with a F-test of `bac08` and `bac10`. If both show insignificance in a multivariate regression but rejects the $H_0$ in a f-test, we should convert it into a binary variable of BAC laws or none

```{r}
tmp = data %>% select(year,totfatrte,sbprim,sbsecon) %>% group_by(year) %>% summarize_all(funs(mean)) %>% mutate(sbnone=1-sbprim-sbsecon) 

tmp=tmp$totfatrte*tmp
tmp$year=tmp$year/sqrt(tmp$totfatrte)
tmp$totfatrte=NULL
rownames(tmp)=tmp$year
tmp$year=NULL
tmp=t(tmp)
barplot(as.matrix(tmp),col=c('blue','green','yellow'),xlab='Year',ylab='Fatalities/100k vehicle miles',main='Fatalities/100k vehicle mile by year and Seatbelt Laws')
legend('topright',legend=rownames(tmp),col=c('blue','green','yellow'),lty=c(1,1,1,1))
```

Seatbelts appear to have a simulatenous decrease with fatalities. It should be included as an independent variable. Much like BAC levels, the mix of seatbelt laws does not appear to affect fatalities. There appears to be slightly lagged effect on the binary seatbelt law - possibly the population is getting in the habit of putting on seatbelts. After most states have implemented at least primary wseatbelt laws (1992-1995), though, the average fatality rate evens off.

We performed t-test for 1999 and 2000 where the percentages are closer to even for sbprim and sbsecon. For both t-tests, $H_0$ is not rejected and there does not appear to be any contemporaneous impact difference between seatbelt laws. Despite the increase in sbprim mix from 1995 to 2004, the fatalities from 1995 to 2004 is similar. There is unlikely to be a lagged impact of seatbelt law differences. Finally, we also note that state 30 does not have seatbelt laws throughout the period.

```{r}
#t.test(data[data$year==1999 & data$sbprim==1,"totfatrte"],data[data$year==1999 & data$sbsecon==1,'totfatrte'])
#t.test(data[data$year==2000 & data$sbprim==1,"totfatrte"],data[data$year==2000 & data$sbsecon==1,'totfatrte'])  
```

```{r fig.height=3}
tmp = data %>% group_by(year) %>% summarize_all(funs(mean))
plotmix(tmp,'minage','Minimum Drinking Age')

tmp2=data %>% group_by(state) %>% summarize_all(funs(mean)) %>% as.data.frame
tmp2=tmp2[tmp2$minage %in% unique(data$minage),c('state','minage')]
tmp2=tmp2[!(tmp2$state %in% c(47,51)),c('state','minage')]
tmp2
```
## These might be better as distinct colors since we only have 4 categories - there are actually quite a few minage - like 5-6 - not sure what's the deal with these - I'll let it as above for now? Suggestions welcome!

Minmum drinking age appears to have some effect on fatalies. Interestingly, all states listed did not have minimum age laws changed during the period. The mean `minage` from 1980-1990 trended higher to 21 in 1990. If we were to focus on `minage`, we can split the data into 2 sets and run separate analysis, detrend and analyze the impact of `minage` on fatalities. We should also interact this variable with BAC variables as raising minimum drinking age may potentially offset some effects of BAC laws. The interaction term is expected to have a negative coefficient.

```{r fig.height=3}
#plotmix(tmp,'zerotol','Zero Tolerance')
tmp=data
tmp$zerotol_bin=ifelse(tmp$zerotol>0,1,0)

tmp = tmp %>% select(year,totfatrte,zerotol_bin) %>% group_by(year) %>% summarize_all(funs(mean)) %>% mutate(zerotol_none=1-zerotol_bin)
tmp=tmp$totfatrte*tmp
tmp$year=tmp$year/sqrt(tmp$totfatrte)
tmp$totfatrte=NULL
rownames(tmp)=tmp$year
tmp$year=NULL
tmp=t(tmp)
barplot(as.matrix(tmp),col=c('blue','red'),xlab='Year',ylab='Fatalities/population',main='Fatalities/population zero toleranace laws mix')
legend('bottomleft',legend=rownames(tmp),col=c('blue','red'),lty=c(1,1,1,1))
```

Zero tolerance laws do not appear to have a contemperous impact on fatalities based on the changes in laws from 1992-1997. It may potentally have a long-tailed effect.

```{r fig.height=3}
#tmp = data %>% group_by(year) %>% summarize_all(funs(mean))
#plotmix(tmp,'gdl','Graduated Driver License')
tmp=data
tmp$gdl_bin=ifelse(tmp$gdl>0,1,0)

tmp = tmp %>% select(year,totfatrte,gdl_bin) %>% group_by(year) %>% summarize_all(funs(mean)) %>% mutate(gdl_none=1-gdl_bin)
tmp=tmp$totfatrte*tmp
tmp$year=tmp$year/sqrt(tmp$totfatrte)
tmp$totfatrte=NULL
rownames(tmp)=tmp$year
tmp$year=NULL
tmp=t(tmp)
barplot(as.matrix(tmp),col=c('blue','red'),xlab='Year',ylab='Fatalities/population',main='Fatalities/population graduated drivers laws mix')
legend('bottomleft',legend=rownames(tmp),col=c('blue','red'),lty=c(1,1,1,1))
```

Graduated driver license laws do not appear to impact fatalies very much. Most likely, it will not impact fatalities even accounting for time lags. The changes from graduated license laws from 1999 to 2004 barely impacted fatalities with or without lag effects.

```{r fig.height=3}
#tmp = data %>% group_by(year) %>% summarize_all(funs(mean))
#plotmix(tmp,'perse','Per Se Law')

tmp=data
tmp$perse_bin=ifelse(tmp$perse>0,1,0)

tmp = tmp %>% select(year,totfatrte,perse_bin) %>% group_by(year) %>% summarize_all(funs(mean)) %>% mutate(perse_none=1-perse_bin)
tmp=tmp$totfatrte*tmp
tmp$year=tmp$year/sqrt(tmp$totfatrte)
tmp$totfatrte=NULL
rownames(tmp)=tmp$year
tmp$year=NULL
tmp=t(tmp)
barplot(as.matrix(tmp),col=c('blue','red'),xlab='Year',ylab='Fatalities/population',main='Fatalities/population Per Se laws mix')
legend('bottomleft',legend=rownames(tmp),col=c('blue','red'),lty=c(1,1,1,1))
```

Per se law does not appear to impact fatalities. The increase from 1982 to 1983 did not appear to have a contemporaneous or lagged impact on fatalities. Per se laws may have interactions with BAC laws as it increasese the "harshness" of bac laws.

```{r fig.height=3}
tmp = data %>% group_by(year) %>% summarize_all(funs(mean))
plotmix(tmp,'vehicmilespc','Vehicle Miles Per Capita')
```

Vehicle miles may be secondarily affected by traffic laws such as graduated license laws. Preliminarly, fatalities appear to decrease as it increases. This makes no sense and is likely to be trending effect through time.

<<<<<<< HEAD
While all states increased vehicmilespc, state 46 was interesting in that there was a large increase in 2001-2001 followed by a large drop back down to historical trend by 2003-2004. Closer, state-specific reasoning may be required. Given that we do not have that information, we will not examine further into it. **SHOW GRAPH**

```{r fig.height=3}
=======
```{r}
>>>>>>> origin/master
plotmix(tmp,'unem','Unemployment')

plot(data$totfatrte, data$unem)
cor(data$totfatrte, data$unem)

conditional_plot(data, data$unem, data$state, "Unemployment Rate by State")
```
```{r}
states <- unique(data$state)

for(s in states){
  d <- data[data$state==s,]
  cor(d$totfatrte, d$unem)
}

# plot high correlation states
```

```{r}
conditional_plot(data, data$totfatrte, data$state, "Total Fatality Rate per 100,000 populartion by State")
```

##Here I think using unique colors would look good. esp since the pattern is more random

Fatalities do not appear to be affected by unemployment rates at all. In fact, the pattern appears random. We expect this variable to have a $\beta$ close to 0 in regressions. If we do include the variable, we may want to consider logging it to spread out the values and improve the regression. It does not appear heteroskedastic.

```{r fig.height=3}
hist(tmp$perc14_24)
hist(log(tmp$perc14_24))
```

The variable is extremely left skewed and a log transformation will be performed. It appears that percent 14-24 in is negatively correlated with fatalities. This makes no sense is it may simply be an artifact of general trend. 

```{r fig.height=3}
#plotmix(tmp,'pe`rc14_24','Percent 14-24')
tmp=data
tmp$logperc14_24=log(data$perc14_24)
tmp = data %>% group_by(year) %>% mutate(logperc=log(perc14_24)) %>% summarize_all(funs(mean))
ggplot(tmp,aes(y=totfatrte,x=year,fill=logperc))+geom_bar(stat='identity')+labs(x='Year',y='Fatalities/population',title='Fatalities/Population vs Percent 14-24')#+theme(plot.title = element_text(hjust = 0.5))+guides(fill=guide_legend(title=text))
```

While all states decreased in their population of 14-24 year olds, a few states increased in the ratio. Most significantly, state 45's increase stood out amongst all the states. Again, without more state specific infomration, it's difficult to further examine the increase. ## PUT IN A GRAPH SHOWING THIS

From the EDA, there are variables that appear to be "incorrectly" correlated with fatalities, such as vehicmilespc. Others such as BAC may be better transformed into a binary on-off variable. Per se, zero tolerance, graduated license, minimum drinking age laws appears to have no effect while seatbelts appear to have a contemporaneous impact on fatalites. Speed Limit laws appear to have a lagged effect.

(Question 2) We will first examine the general time trend of fatalities. Recall that our fatalities variable, `totfatrte`, is Fatalities per 100,000 population is already  normalized by population, so proper analysis of impact of traffic laws on fatalities can be analyzed. Note that traffic laws are most likely uncorrelated with state population as shown below. Generally, states laws have lower correlation with population suggesting that has less impact than vehicle miles, which may be affected by state laws. Therefore, total fatalities adjusted by population is probably a better variable to infer state law causalities.

## SS we can include a chart with state total fatality rate v popultion? that would confirm there's not relationship.

```{r}
cor(data[,c('statepop','vehicmiles','minage','zerotol','gdl','seatbelt')])
```

```{r}
tmp=data %>% select(year,totfatrte) %>% group_by(year) %>% summarize_all(funs(mean))
data.frame('Year'=tmp$year,'Fatalities/Population'=tmp$totfatrte)
ggplot(data %>% select(year,totfatrte) %>% group_by(year) %>% summarize_all(funs(mean)),aes(year,totfatrte,label=totfatrte))+geom_line(color='red')+geom_point(color='blue')+geom_text(aes(label=round(totfatrte,1)),color='black',hjust=0, vjust=-0.5)

tmp = data %>% select(year,totfatrte) %>% group_by(year) %>% summarise_all(funs(mean))
ggplot(tmp,aes(year,totfatrte,label=totfatrte))+geom_line()+geom_point(aes(col='red'))+geom_label_repel(aes(label = round(totfatrte,2)), box.padding   = 0.35,  point.padding = 0.5, segment.color = 'grey50') + theme_classic()
```

# PLEASE NOTE BELOW - I THINK WE'RE SUPPOSED TO USE THE PLM
```{r}
library(plm)
m=lm(totfatrte~factor(year),data)
summary(m)
```

(Question 2) Regression of fatalities vs each year shows that there is a clear significant downward trend. The $F-test$ p-value of $\sim 0$ shows that the dummy variables for year is jointly significant. The regression suggests that fatalities have been decreasing through time and the $\beta$s show the mean differential between the year $t$ and 1980. The intercept of the regression is the mean fatalities in 1980 and the coefficients is the mean differences from 1980 for each year respectively. Notice the 2 chart are exact same shape after the 1st year (1980). Driving became much safer over time. Note that it's a general trend over time as the +/-95% confidence intervals of $\beta$s overlap each other in the surrounding years. We can only conclude that the trend has decreased but difficult to firmly draw conclusions on any given year.

```{r}
tmp=data.frame(confint(m))
tmp=tmp[2:dim(tmp)[1],]
colnames(tmp)=c('lower','upper')
tmp$year=1981:2004
tmp$beta=(tmp$lower+tmp$upper)/2

ggplot(tmp,aes(x=year,y=beta))+geom_point(size=2.5,color='red')+geom_line()+geom_errorbar(aes(ymax=tmp$upper,ymin=tmp$lower)) +labs('title'='Coefficients for fatality by Year (Mean difference from 1980)',x='Year',y="Fatality difference from 1980")
```

Note that the residuals are not normally distributed and fails the Shapiro Wilks test.

```{r}
qqnorm(m$residuals)
qqline(m$residuals,col='red')
shapiro.test(m$residuals)
```

(Question 3) We will now expand the previous regression with additional regressors - bac08, bac10, perse, sbprim, sbsecon, sl70plus, gld, perc14_24, unem and vehicmilespc. While all variables does not appear to be heteroskedastic for log transforms, perc14-24 is logged since it is very left skewed and unem is also logged. Logging the values spreads the values out for better a better regression. vehicmilepc does not appear to require transformations as it appears more normally/uniformly distributed. The rest of variables are binary variables and no transformations are done. BAC and speed limit variables are not binarized and no interactions are implemented as found through the EDA due to the limited space available.

```{r}
m=lm(totfatrte~factor(year)+bac08+bac10+perse+sbprim+sbsecon+sl70plus+gdl+log(perc14_24)+log(unem)+vehicmilespc,data)
summary(m)
```

bac08 and bac10 are coefficients of `r data.frame(t(m$coefficients))$bac08` and `r data.frame(t(m$coefficients))$bac10`. The $\beta_{bac08}$ and $\beta_{bac10}$ represent the impact of having bac08 and bac10 laws in that year (regardless of the year) on the *mean* fatalities across the states. Per se laws also decrease the mean fatalities by `r data.frame(t(m$coefficients))$perse` once it's enacted. Primay seat belt laws does not seem to have an impact on fatalities despite the $\beta_{sbprim}=-0.077$. We note that all the $\beta$ estimators, p-values and other regression statistics above are biased and invalid as the residuals are serially correlated. In reality, no conclusion or inference can be made with the estimator since the regression assumptions are violated.

```{r}
durbinWatsonTest(m)
```

Additionally, we can do a pool test to check if the coefficients are consistent vs a fixed effects model, and the hypothesis is rejected showing that the estimates are not consistent either (we can take 1 cross-sectional regressional and obtain consistent estimators *if* unobserved effects are uncorrelated with explanatory variables).

```{r}
library(plm)
data.p=pdata.frame(data,index = c('state','year'))
pooltest(totfatrte~bac08+bac10+perse+sbprim+sbsecon+sl70plus+gdl+log(perc14_24)+log(unem)+vehicmilespc,data = data.p,model='within')
```

```{r}
#library(plm)
#data.p=pdata.frame(data,index = c('state','year'))
#m.pool=plm(totfatrte~factor(year)+bac08+bac10+perse+sbprim+sbsecon+sl70plus+gdl+log(perc14_24)+log(unem)+vehicmilespc,data = data.p,model='pooling')
#summary(m.pool)
```

(Question 4) Given the pooled model violates OLS assumptions, we will examine data using a fixed effect model.
```{r}
m.fe=plm(totfatrte~bac08+bac10+perse+sbprim+sbsecon+sl70plus+gdl+log(perc14_24)+log(unem)+vehicmilespc,data = data.p,model='within')
summary(m.fe)
```

```{r}
# should add p-Value or SE in here
d=data.frame(pooled_coef=summary(m)$coefficients[26:35,1],
             pooled_SE=summary(m)$coefficients[26:35,2],
             FE_coef=summary(m.fe)$coefficients[,1],
             FE_SE=summary(m.fe)$coefficients[,2]) 
d
```

The coefficients are significantly different between the pooled OLS and Fixed Effects regression. FE model is better since it removes the fixed effects. Pooled OLS assumes that there is no correlation between unobserved variable and any of the regressors. This assumption is clearly broken. For example, dry laws, which are unobserved, may be correlated with bac08 laws and affect fatalities. Even if the assumption is not broken, the serial correlation in the composite error is not accounted for in pooled OLS. The standarded errors in a pooled OLS are incorrect as are statistical tests. In addition to biased estimators, note the SE of pooled OLS are much to tight since it assumes we have more observations than we really do with only $t=25$. For the FE models, the assumption is that the idiosyncratic errors are uncorrelated conditional on the indepdendent variables and time-invariant unobservable variables. Given the current context, it the FE assumptions are more reasonable as time-invariant error can be eliminated.

(Question 5) In comparing FE models with RE models, FE models is likely to be a better estimate in the current context. Like the Pooled OLS, RE model assumes no correlation between fixed effects and independent variables. The difference between the 2 models is that RE corrects the serial correlation within the composite error by estimating a correlation. The advantage of RE models over FE is the ability to estimate time-invariant variables. However, it also requires an extremely strong assumption that fixed effects are independent of all explanatory variables across all time periods. Given the endogeneity issues, we believe fixed effects is a much better model than random effects. We can run a Hausman test where the $H_0$ is that the unique errors are not correlated with the regressors. The $H_0$ is rejected in the Hausman test.

```{r}
m.re=plm(totfatrte~bac08+bac10+perse+sbprim+sbsecon+sl70plus+gdl+log(perc14_24)+log(unem)+vehicmilespc,data = data.p,model='random')
phtest(m.fe,m.re)
```
```{r}
summary(m.fe)

var1 <- data.frame(t(m.fe$coefficients))$vehicmilespc
var2 <- data.frame(t(confint(m.fe)))$vehicmilespc
```
# WRITE THE FE MODEL

$$
y_{it}-\bar{y}_i= -1.95 (x_{(bac08)it}-\bar{x}_{bac08}) +  -1.56 (x_{(bac10)it}-\bar{x}_{bac10}) -1.56 (x_{(per se)it}-\bar{x}_{per se}) + -1.80 (x_{(sbprim)it}-\bar{x}_{sbprim}) + -0.86(x_{(sbsecon)it}-\bar{x}_{sbsecon}) + -1.12 (x_{(sl70plus)it}-\bar{x}_{sl70plus})  + -0.59 (x_{(sl70plus)it}-\bar{x}_{sl70plus} + 14.66(x_{(log(per14_24))it}-\bar{x}_{log(perc14_24)})+-0.59(x_{(log(unem))it}-\bar{x}_{log(unem)})+0.0003(x_{(vehicmilespc)it}-\bar{x}_{vehicmilespc})
$$

```{r}
tmp=data %>% group_by(year) %>% summarize_all(funs(mean)) %>% select(vehicmilespc) %>% as.data.frame
tmp=mean(diff(tmp))
```

(Question 6) If $vehicmilepc$ increase by 1,000 in a time period $t$ assuming the $\bar{vehicmilepc}$ does not change form the FE model, $totfatrte$ is expected to increase `r var1` with a 95% confidence interval of `r var2`. The $\beta_{vehicmilespc}$ is not a very precise estimate. The reason is that there is minimal time-variance on the variable. The mean variance across the years on vehicmilespc is `r tmp` while the average is `r mean(data$vehicmilespc)`. The small variation across time causes the estimate to be imprecise. For example, a 10% increase in $vehicmilespc$ has a 95% confidence interval of $0.114$ and $0.48$ $totfatrte$, which would roughly be a 5% difference in $totfatret$ between the lower and upper bound.

(Question 7) Assuming we have no omitted variable bias and autocorrelation and heteroskedasticity exists in errors, it causes your estimators to be inefficient while still unbiased. The standard errors estimated on the estimators are incorrect as the variance of the estimators is inflated (heteroskedasticity). It is inefficient as it is no longer the minimum variance estimator.

**Exercises:**

1. Load the data. Provide a description of the basic structure of the dataset, as we have done throughout the semester. Conduct a very thorough EDA, which should include both graphical and tabular techniques, on the dataset, including both the dependent variable *totfatrte* and the potential explanatory variables. You need to write a detailed narrative of your observations of your EDA. *Reminder: giving an "output dump" (i.e. providing a bunch of graphs and tables without description and hoping your audience will interpret them) will receive a zero in this exercise.*

2. How is the our dependent variable of interest *totfatrte* defined? What is the average of this variable in each of the years in the time period covered in this dataset? Estimate a linear regression model of *totfatrte* on a set of dummy variables for the years 1981 through 2004. What does this model explain? Describe what you find in this model. Did driving become safer over this period? Please provide a detailed explanation.

```{r}
lm1 <- lm(totfatrte ~ d81 + d82 + d83 + d84 + d85 + d86 + d87 + d88 + d89 + d90 + d91 + d92 + d93 + d94 + d95 + d96 + d97 + d98 + d99 + d00 + d01 + d02 + d03 + d04, data)

lm1
```

3. Expand your model in *Exercise 2* by adding variables *bac08, bac10, perse, sbprim, sbsecon, sl70plus, gdl, perc14_24, unem, vehicmilespc*, and perhaps *transformations of some or all of these variables*. Please explain carefully your rationale, which should be based on your EDA, behind any transformation you made. If no transformation is made, explain why transformation is not needed. How are the variables *bac8* and *bac10* defined? Interpret the coefficients on *bac8* and *bac10*. Do *per se laws* have a negative effect on the fatality rate? What about having a primary seat belt law? (Note that if a law was enacted sometime within a year the fraction of the year is recorded in place of the zero-one indicator.)

```{r}
lm2 <- lm(totfatrte ~ d81 + d82 + d83 + d84 + d85 + d86 + d87 + d88 + d89 + d90 + d91 + d92 + d93 + d94 + d95 + d96 + d97 + d98 + d99 + d00 + d01 + d02 + d03 + d04 + bac08 + bac10 + perse + sbprim + sbsecon + sl70plus + gdl + perc14_24 + unem + vehicmilespc, data)

lm2
```

4. Reestimate the model from *Exercise 3* using a fixed effects (at the state level) model. How do the coefficients on *bac08, bac10, perse, and sbprim* compare with the pooled OLS estimates? Which set of estimates do you think is more reliable? What assumptions are needed in each of these models?  Are these assumptions reasonable in the current context?

```{r}
plm(totfatrte ~ d81 + d82 + d83 + d84 + d85 + d86 + d87 + d88 + d89 + d90 + d91 + d92 + d93 + d94 + d95 + d96 + d97 + d98 + d99 + d00 + d01 + d02 + d03 + d04 + bac08 + bac10 + perse + sbprim + sbsecon + sl70plus + gdl + perc14_24 + unem + vehicmilespc, data)
```


5. Would you perfer to use a random effects model instead of the fixed effects model you built in *Exercise 4*? Please explain.

Fixed effects are constant across the states and this may most likely not true, considering our EDA. Random effects vary across the states, and this is more likely to be close to reality.

6. Suppose that *vehicmilespc*, the number of miles driven per capita, increases by $1,000$. Using the FE estimates, what is the estimated effect on *totfatrte*? Please interpret the estimate.

7. If there is serial correlation or heteroskedasticity in the idiosyncratic errors of the model, what would be the consequences on the estimators and their standard errors?




